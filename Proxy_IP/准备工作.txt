
# 参照崔大神的代理池搭建，主要是以前的IP地址网站很多不能用了，自己又从新找的一些IP网站，基本还是遵从原作者的代码
# 每一行代码都有详细的汉语注释

# 1.基本模块分为4块
#         存储模块    获取模块    检查模块    接口模块
#         1.存储模块：首先要保证代理不重复，标识代理可用情况，动态处理每个代理，使用Redis数据库的SortedSet(有序集合)
#         2.获取模块：代理形式都是 IP + 端口，尽量抓高匿名代理，将抓取成功的代理保存到Redis数据库中
#         3.检测代理：需要定时检测数据库中的代理，这里需要设置一个检测链接，最好是爬取那个网站就检测那个网站，这样更有
#                    针对性，如果要做一个通用型的代理，那可以设置百度等链接来检测。另外我们需要标识每一个代理的状态，
#                   如设置分数标识，100分代表可用，分数越少代表越不可用，检测一次，如果代理可用，我们将分数标识立即设置为
#                    100满分，也可以原基础上 + 1分，如果代理不可用，可以将分数—1分，当分数达到一定阈值后，代理就直接从
#                    数据库中移除，通过这样的标识分数，我们就可以辨别代理的可用情况，选用的时候会更有针对性。
#        4.接口模块：其实我们可以直接连数据库来取对应的数据，但是这样就需要知道数据库的连接信息，并配置数据库，
#                    而比较安全且方便的方式就是提供一个 Web API接口，我们通过访问接口就可以拿到可用代理，并随机返回


########################################用法说明################################################
# 首先你得安装 redis数据库，aiohttp模块，requests模块，pyquery解析库，Flash库，

# 爬取的时候只需要在 my_settings_proxy.py 文件里面定义目标网站，以及自行删除添加 my_crawler_ip.py 文件当中的以“crawl_”
# 开头的方法，启动代理池在 my_run.py 文件。开启代理池后，调用call_proxy文件夹下的call_proxies.py文件中的 get_proxies()方法
# 即可获取随机代理。
